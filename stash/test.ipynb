{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import time\n",
    "import torch\n",
    "import MinkowskiEngine as ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/mavovk/TestTask/')  # path to repository directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_ransac_registration(source_down, target_down, source_feats, target_feats, voxel_size):\n",
    "    distance_threshold = voxel_size * 1.5\n",
    "\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_feats, target_feats, True, distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False), 4,\n",
    "        [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9)\n",
    "        ],\n",
    "        o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999)\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pcd_from_array(array):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(array)\n",
    "\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fcgf(model,\n",
    "                xyz,\n",
    "                rgb=None,\n",
    "                normal=None,\n",
    "                voxel_size=0.05,\n",
    "                device=None,\n",
    "                skip_check=False,\n",
    "                is_eval=True):\n",
    "    if is_eval:\n",
    "        model.eval()\n",
    "\n",
    "    if not skip_check:\n",
    "        assert xyz.shape[1] == 3\n",
    "\n",
    "        N = xyz.shape[0]\n",
    "        if rgb is not None:\n",
    "            assert N == len(rgb)\n",
    "            assert rgb.shape[1] == 3\n",
    "\n",
    "            if np.any(rgb > 1):\n",
    "                raise ValueError('Invalid color. Color must range from [0, 1]')\n",
    "\n",
    "        if normal is not None:\n",
    "            assert N == len(normal)\n",
    "            assert normal.shape[1] == 3\n",
    "\n",
    "            if np.any(normal > 1):\n",
    "                raise ValueError('Invalid normal. Normal must range from [-1, 1]')\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    feats = []\n",
    "    if rgb is not None:\n",
    "        # [0, 1]\n",
    "        feats.append(rgb - 0.5)\n",
    "\n",
    "    if normal is not None:\n",
    "        # [-1, 1]\n",
    "        feats.append(normal / 2)\n",
    "\n",
    "    if rgb is None and normal is None:\n",
    "        feats.append(np.ones((len(xyz), 1)))\n",
    "\n",
    "    feats = np.hstack(feats)\n",
    "\n",
    "    # Voxelize xyz and feats\n",
    "    coords = np.floor(xyz / voxel_size)\n",
    "    coords, inds = ME.utils.sparse_quantize(coords, return_index=True)\n",
    "    # Convert to batched coords compatible with ME\n",
    "    coords = ME.utils.batched_coordinates([coords])\n",
    "    return_coords = xyz[inds]\n",
    "\n",
    "    feats = feats[inds]\n",
    "\n",
    "    feats = torch.tensor(feats, dtype=torch.float32)\n",
    "    coords = torch.tensor(coords, dtype=torch.int32)\n",
    "\n",
    "    stensor = ME.SparseTensor(feats, coordinates=coords, device=device)\n",
    "\n",
    "    return make_pcd_from_array(return_coords), model(stensor).F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_fcgf_registration(source, target, voxel_size, model, device):\n",
    "    global preprocess_time\n",
    "    global registration_time\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    source_down, source_fcgf = extract_fcgf(model, source, rgb=None, normal=None, voxel_size=voxel_size, device=device, skip_check=True)\n",
    "    target_down, target_fcgf = extract_fcgf(model, target, rgb=None, normal=None, voxel_size=voxel_size, device=device, skip_check=True)\n",
    "\n",
    "    preprocess_time.append(time.time() - start)\n",
    "\n",
    "    registration = None\n",
    "    start = time.time()\n",
    "\n",
    "    registration = execute_ransac_registration(source_down, target_down, source_fcgf, target_fcgf, voxel_size)\n",
    "\n",
    "    registration_time.append(time.time() - start)\n",
    "    return registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_model\n",
    "\n",
    "def get_model(model_path, device):\n",
    "    checkpoint =  torch.load(model_path, weights_only=False)\n",
    "    config = checkpoint['config']\n",
    "\n",
    "    num_feats = 1\n",
    "    Model = load_model(config.model)\n",
    "\n",
    "    model = Model(num_feats, config.model_n_out, bn_momentum=0.05, normalize_feature=config.normalize_feature,\n",
    "                conv1_kernel_size=config.conv1_kernel_size, D=3)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../FCGF/weights/3dmatch.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04799998,  0.21599996,  0.80000001],\n",
       "       [ 0.05400002,  0.21599996,  0.80000001],\n",
       "       [ 0.06000006,  0.21599996,  0.80000001],\n",
       "       ...,\n",
       "       [-0.44400001, -1.32599998,  3.49399996],\n",
       "       [-0.43799996, -1.32599998,  3.49399996],\n",
       "       [-0.426     , -1.31999993,  3.49399996]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(points3d.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_down, pcd_features = get_global_fcgf_registration(source, target, voxel_size, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6342792 ,  0.60562277,  0.35333985],\n",
       "       [ 0.6449546 ,  0.5971641 ,  0.35637185],\n",
       "       [ 0.62937343,  0.6095967 ,  0.3248829 ],\n",
       "       ...,\n",
       "       [-2.2738626 , -7.9960003 , -2.2363813 ],\n",
       "       [-2.3241282 , -7.997     , -2.236661  ],\n",
       "       [-2.3613188 , -7.9960003 , -2.2363813 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163045/1872968029.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(coords, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "source_down, source_fcgf = extract_fcgf(model, ref_cloud, rgb=None, normal=None, voxel_size=0.2, device=device, skip_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pcd(cloud, voxel_size):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud)\n",
    "\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    normal_radius = 2 * voxel_size\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=normal_radius, max_nn=30)\n",
    "    )\n",
    "\n",
    "    feature_radius = 5 * voxel_size\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=feature_radius, max_nn=100)\n",
    "    )\n",
    "\n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_down, source_fpfh =  preprocess_pcd(ref_cloud, voxel_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open3d.cuda.pybind.pipelines.registration.Feature"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(source_fpfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31693539,  0.22424328,  0.10209992, ..., -0.08902992,\n",
       "         0.24003176, -0.18045999],\n",
       "       [-0.33956698,  0.24904642,  0.12067857, ..., -0.08081627,\n",
       "         0.21730782, -0.1717004 ],\n",
       "       [-0.31574926,  0.20526516,  0.0807789 , ..., -0.13470003,\n",
       "         0.27149224, -0.17523108],\n",
       "       ...,\n",
       "       [-0.17566216,  0.13519622,  0.2466822 , ...,  0.02478226,\n",
       "        -0.10432456,  0.02682614],\n",
       "       [ 0.01471485,  0.18719289,  0.2233877 , ...,  0.05365718,\n",
       "        -0.04523702,  0.07966842],\n",
       "       [-0.11011513,  0.15812801,  0.24468422, ...,  0.02087312,\n",
       "         0.00527604,  0.11162176]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_fcgf.detach().cpu().numpy().astype('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3918, 32])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_fcgf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m feature \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mpipelines\u001b[38;5;241m.\u001b[39mregistration\u001b[38;5;241m.\u001b[39mFeature()\n\u001b[1;32m      2\u001b[0m feature\u001b[38;5;241m.\u001b[39mresize(source_fcgf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], source_fcgf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m feature\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43msource_fcgf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "feature = o3d.pipelines.registration.Feature()\n",
    "feature.resize(source_fcgf.shape[1], source_fcgf.shape[0])\n",
    "feature.data = source_fcgf.detach().numpy().transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcgf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
